# src/utils/cache_manager.py
"""
Central cache manager with persistent disk backing and LRU eviction.

Behavior:
- Prefers 'diskcache' library if available (recommended).
- If 'diskcache' is not installed, falls back to a robust file-backed cache implemented
  with pickled values and a JSON index that tracks sizes and last-access timestamps,
  and performs LRU eviction to satisfy size limits.

API (CacheManager):
    - CacheManager(cache_dir, size_limit=None)
    - get(key_obj, default=None)
    - set(key_obj, value, expire_seconds=None)
    - exists(key_obj)
    - delete(key_obj)
    - clear()
    - cache_function(expire_seconds=None): decorator for functions

Notes:
- Keys are generated by hash_key(obj) = sha256(pickle.dumps(obj))
- Values are pickled; ensure stored values are pickle-able (numpy arrays, dicts, floats, ndarrays are fine).
"""

from __future__ import annotations
import os
import time
import json
import pickle
import hashlib
import numpy as np
from typing import Any, Optional, Callable
import tempfile

# Try to import diskcache (preferred) ------------------------------------------------
# Try to import diskcache (preferred) ------------------------------------------------
try:
    import diskcache as dc  # type: ignore

    _HAS_DISKCACHE = True
except Exception:
    _HAS_DISKCACHE = False


# ---------------------------------------------------------------------
# Utility helpers
# ---------------------------------------------------------------------
def hash_key(obj: Any) -> str:
    """
    Produce a stable hex key for any pickle-able Python object using SHA-256 of pickle payload.
    """
    payload = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL)
    return hashlib.sha256(payload).hexdigest()


def _ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)


def _short_hash_bytes(b: bytes, n_hex: int = 16) -> str:
    """Return a short hex digest of bytes for readable cache keys."""
    return hashlib.sha256(b).hexdigest()[:n_hex]


def _bytes_key_of_array(a: np.ndarray) -> bytes:
    """Stable bytes representation for numpy array used in cache keys."""
    arr = np.ascontiguousarray(a)
    return arr.tobytes()


# ---------------------------------------------------------------------
# File-backed fallback implementation
# ---------------------------------------------------------------------
class _FileCacheFallback:
    """
    Simple file-backed cache with an index (JSON) and LRU eviction.

    Directory layout:
      cache_dir/
        index.json
        values/
           <key>.pkl
    """

    INDEX_NAME = "index.json"
    VALUES_DIR = "values"

    def __init__(self, cache_dir: str, size_limit_bytes: Optional[int] = None):
        self.cache_dir = os.path.abspath(cache_dir)
        _ensure_dir(self.cache_dir)
        self.values_dir = os.path.join(self.cache_dir, self.VALUES_DIR)
        _ensure_dir(self.values_dir)
        self.index_path = os.path.join(self.cache_dir, self.INDEX_NAME)
        self.size_limit_bytes = (
            None if size_limit_bytes is None else int(size_limit_bytes)
        )
        # self._lock = threading.Lock() # Removed due to hanging issues in pytest
        # index: { key -> { 'filename': str, 'size': int, 'atime': float } }
        self._index = self._load_index()

    def _load_index(self):
        if os.path.exists(self.index_path):
            try:
                with open(self.index_path, "rb") as f:
                    return json.load(f)
            except Exception:
                # corrupted index, reset
                return {}
        return {}

    def _save_index(self):
        tmp = self.index_path + ".tmp"
        with open(tmp, "wb") as f:
            f.write(json.dumps(self._index).encode("utf-8"))
        os.replace(tmp, self.index_path)

    def _value_path(self, key: str) -> str:
        return os.path.join(self.values_dir, f"{key}.pkl")

    def exists(self, key: str) -> bool:
        return key in self._index and os.path.exists(self._index[key]["filename"])

    def get(self, key: str, default: Any = None) -> Any:
        if key not in self._index:
            return default
        entry = self._index[key]
        try:
            with open(entry["filename"], "rb") as f:
                val = pickle.load(f)
            # update atime
            entry["atime"] = time.time()
            self._save_index()
            return val
        except Exception:
            # missing or corrupted file: remove entry
            try:
                os.remove(entry["filename"])
            except Exception:
                pass
            del self._index[key]
            self._save_index()
            return default

    def set(self, key: str, value: Any, expire_seconds: Optional[int] = None):
        data = pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)
        p = self._value_path(key)
        tmp = p + ".tmp"
        # write file atomically
        with open(tmp, "wb") as f:
            f.write(data)
        os.replace(tmp, p)
        size = os.path.getsize(p)
        self._index[key] = {"filename": p, "size": size, "atime": time.time()}
        self._save_index()
        # perform eviction if size limit exceeded
        if self.size_limit_bytes is not None:
            self._enforce_size_limit()

    def delete(self, key: str):
        if key in self._index:
            fname = self._index[key]["filename"]
            try:
                os.remove(fname)
            except Exception:
                pass
            del self._index[key]
            self._save_index()

    def clear(self):
        # remove all files in values_dir
        for key, entry in list(self._index.items()):
            try:
                os.remove(entry["filename"])
            except Exception:
                pass
            self._index.pop(key, None)
        self._save_index()

    def size_bytes(self) -> int:
        return sum(entry["size"] for entry in self._index.values())

    def _enforce_size_limit(self):
        """Evict least-recently-used entries until size <= limit."""
        if self.size_limit_bytes is None:
            return
        total = self.size_bytes()
        if total <= self.size_limit_bytes:
            return
        # sort keys by atime ascending (oldest first)
        items = sorted(self._index.items(), key=lambda kv: kv[1]["atime"])
        for key, entry in items:
            try:
                os.remove(entry["filename"])
            except Exception:
                pass
            total -= entry["size"]
            del self._index[key]
            if total <= self.size_limit_bytes:
                break
        self._save_index()


# ---------------------------------------------------------------------
# Primary CacheManager: wrapper that chooses backend
# ---------------------------------------------------------------------
class CacheManager:
    """
    Cache manager wrapper.

    Args:
      cache_dir: path for persistent cache.
      size_limit_bytes: if provided, attempt to limit disk usage to this many bytes (LRU eviction).
    """

    def __init__(
        self,
        cache_dir: Optional[str] = None,
        size_limit_bytes: Optional[int] = None,
        use_diskcache_if_available: bool = True,
    ):
        if cache_dir is None:
            cache_dir = os.path.join(tempfile.gettempdir(), "project_cache")
        self.cache_dir = os.path.abspath(cache_dir)
        _ensure_dir(self.cache_dir)
        self.size_limit_bytes = (
            None if size_limit_bytes is None else int(size_limit_bytes)
        )

        # prefer diskcache if available
        if use_diskcache_if_available and _HAS_DISKCACHE:
            # diskcache.Cache accepts size_limit in bytes and default eviction policy is LRU
            self._backend = "diskcache"
            # If size_limit_bytes is None, use diskcache default (1GB) or a large number.
            # diskcache default is 1GB, but if we pass None it might cause issues as seen.
            # Let's pass a default of 1GB if None.
            limit = (
                self.size_limit_bytes
                if self.size_limit_bytes is not None
                else 1073741824
            )
            self._cache = dc.Cache(self.cache_dir, size_limit=limit)
        else:
            # fallback to file-based cache
            self._backend = "file"
            self._cache = _FileCacheFallback(
                self.cache_dir, size_limit_bytes=self.size_limit_bytes
            )

    def _to_key(self, key_obj: Any) -> str:
        """Return hex digest key (string)."""
        if (
            isinstance(key_obj, str)
            and len(key_obj) == 64
            and all(c in "0123456789abcdef" for c in key_obj)
        ):
            # already a hex key
            return key_obj
        return hash_key(key_obj)

    def get(self, key_obj: Any, default: Any = None) -> Any:
        k = self._to_key(key_obj)
        if self._backend == "diskcache":
            val = self._cache.get(k, default=default)
            return val
        else:
            return self._cache.get(k, default=default)

    def set(self, key_obj: Any, value: Any, expire_seconds: Optional[int] = None):
        k = self._to_key(key_obj)
        if self._backend == "diskcache":
            # diskcache accepts expire in seconds as 'expire'
            self._cache.set(k, value, expire=expire_seconds)
        else:
            self._cache.set(k, value, expire_seconds)

    def exists(self, key_obj: Any) -> bool:
        k = self._to_key(key_obj)
        if self._backend == "diskcache":
            return k in self._cache
        else:
            return self._cache.exists(k)

    def delete(self, key_obj: Any):
        k = self._to_key(key_obj)
        if self._backend == "diskcache":
            try:
                del self._cache[k]
            except KeyError:
                pass
        else:
            self._cache.delete(k)

    def clear(self):
        if self._backend == "diskcache":
            self._cache.clear()
        else:
            self._cache.clear()

    def cache_function(self, expire_seconds: Optional[int] = None):
        """
        Decorator to cache function call results. Key is generated from (func.__qualname__, args, kwargs).
        Example:
            @cache_manager.cache_function(expire_seconds=3600)
            def expensive(a, b): ...
        """

        def decorator(func: Callable):
            def wrapper(*args, **kwargs):
                key_obj = (func.__module__ + "." + func.__qualname__, args, kwargs)
                k = self._to_key(key_obj)
                # try get
                val = self.get(k, default=None)
                if val is not None:
                    return val
                res = func(*args, **kwargs)
                self.set(k, res, expire_seconds)
                return res

            wrapper.__wrapped__ = func
            wrapper.__name__ = func.__name__
            wrapper.__doc__ = func.__doc__
            return wrapper

        return decorator

    def stats(self) -> dict:
        """Return basic stats about cache."""
        if self._backend == "diskcache":
            return {
                "backend": "diskcache",
                "curr_size": self._cache.volume(),
                "dir": self.cache_dir,
            }
        else:
            return {
                "backend": "file",
                "curr_size": self._cache.size_bytes(),
                "dir": self.cache_dir,
            }


# =======================================================================
# Very small test when module run directly
# =======================================================================
if __name__ == "__main__":
    cm = CacheManager(cache_dir="./.test_cache", size_limit_bytes=1024 * 1024)
    key = ("test", 1, 2)
    print("setting key", key)
    cm.set(key, {"a": 1, "arr": [1, 2, 3]})
    print("exists?", cm.exists(key))
    v = cm.get(key)
    print("value:", v)
    cm.clear()
    print("cleared; exists?", cm.exists(key))
